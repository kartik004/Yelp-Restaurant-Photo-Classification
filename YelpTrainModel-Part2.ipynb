{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load saved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def loadNew():\n",
    "    f = h5py.File(\"yelpdataset12345.h5\")\n",
    "    x = f['x'].value\n",
    "    y = f['y'].value\n",
    "    f.close()\n",
    "    x_train , x_testInitial, y_train, y_testInitial = train_test_split(x,y,test_size=0.3,random_state=100)\n",
    "    \n",
    "    x_val , x_test, y_val, y_test = train_test_split(x_testInitial,y_testInitial,test_size=0.5,random_state=100)\n",
    "    return x_train, x_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#from getdata import load\n",
    "from keras.models import Sequential\n",
    "from getdata import load\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import hamming_loss\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "x_train, x_val, y_train, y_val = loadNew()\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_val  = x_val.astype('float32')\n",
    "\n",
    "x_train /= 255\n",
    "x_val /= 255\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.10588235,  0.12156863,  0.07843138, ...,  0.08627451,\n",
       "           0.11764706,  0.11764706],\n",
       "         [ 0.08235294,  0.09019608,  0.10588235, ...,  0.14509805,\n",
       "           0.08235294,  0.09411765],\n",
       "         [ 0.06666667,  0.07843138,  0.09019608, ...,  0.08627451,\n",
       "           0.10980392,  0.14509805],\n",
       "         ..., \n",
       "         [ 0.15686275,  0.12941177,  0.11372549, ...,  0.14901961,\n",
       "           0.11764706,  0.11764706],\n",
       "         [ 0.09411765,  0.08235294,  0.09803922, ...,  0.12156863,\n",
       "           0.16862746,  0.15294118],\n",
       "         [ 0.07058824,  0.07058824,  0.10980392, ...,  0.13725491,\n",
       "           0.10588235,  0.09019608]],\n",
       "\n",
       "        [[ 0.03921569,  0.04705882,  0.02745098, ...,  0.06666667,\n",
       "           0.05882353,  0.01960784],\n",
       "         [ 0.03137255,  0.02352941,  0.05098039, ...,  0.1254902 ,\n",
       "           0.10196079,  0.07843138],\n",
       "         [ 0.03921569,  0.02352941,  0.02352941, ...,  0.05098039,\n",
       "           0.05490196,  0.05098039],\n",
       "         ..., \n",
       "         [ 0.05882353,  0.07450981,  0.07450981, ...,  0.08235294,\n",
       "           0.11372549,  0.10980392],\n",
       "         [ 0.08235294,  0.05098039,  0.05882353, ...,  0.07058824,\n",
       "           0.10196079,  0.08627451],\n",
       "         [ 0.07058824,  0.05490196,  0.07058824, ...,  0.07058824,\n",
       "           0.05098039,  0.06666667]],\n",
       "\n",
       "        [[ 0.05098039,  0.05882353,  0.03529412, ...,  0.1254902 ,\n",
       "           0.11372549,  0.07843138],\n",
       "         [ 0.03921569,  0.03529412,  0.05882353, ...,  0.23921569,\n",
       "           0.18431373,  0.16470589],\n",
       "         [ 0.04313726,  0.03137255,  0.03529412, ...,  0.10196079,\n",
       "           0.10196079,  0.10196079],\n",
       "         ..., \n",
       "         [ 0.09803922,  0.09803922,  0.09803922, ...,  0.11764706,\n",
       "           0.15294118,  0.15686275],\n",
       "         [ 0.07058824,  0.08235294,  0.10588235, ...,  0.11764706,\n",
       "           0.13725491,  0.12156863],\n",
       "         [ 0.12941177,  0.10196079,  0.10980392, ...,  0.12156863,\n",
       "           0.05882353,  0.07058824]]],\n",
       "\n",
       "\n",
       "       [[[ 0.40392157,  0.19215687,  0.80784315, ...,  0.07843138,\n",
       "           0.10196079,  0.08235294],\n",
       "         [ 0.76078433,  1.        ,  0.99607843, ...,  0.41960785,\n",
       "           0.51764709,  0.14901961],\n",
       "         [ 1.        ,  1.        ,  1.        , ...,  0.35294119,\n",
       "           0.59607846,  0.66274512],\n",
       "         ..., \n",
       "         [ 0.82745099,  0.83137256,  0.85490197, ...,  0.5529412 ,\n",
       "           0.58823532,  0.59215689],\n",
       "         [ 0.80392158,  0.85882354,  0.82745099, ...,  0.58823532,\n",
       "           0.5529412 ,  0.56862748],\n",
       "         [ 0.83137256,  0.81568629,  0.83529413, ...,  0.57647061,\n",
       "           0.57254905,  0.58431375]],\n",
       "\n",
       "        [[ 0.38039216,  0.16470589,  0.80000001, ...,  0.06666667,\n",
       "           0.09803922,  0.07843138],\n",
       "         [ 0.74117649,  0.98823529,  0.98039216, ...,  0.42745098,\n",
       "           0.50980395,  0.14117648],\n",
       "         [ 1.        ,  0.99607843,  1.        , ...,  0.3764706 ,\n",
       "           0.59215689,  0.65882355],\n",
       "         ..., \n",
       "         [ 0.8392157 ,  0.83529413,  0.8509804 , ...,  0.57647061,\n",
       "           0.60784316,  0.61176473],\n",
       "         [ 0.83137256,  0.87843138,  0.8392157 , ...,  0.61176473,\n",
       "           0.58823532,  0.60392159],\n",
       "         [ 0.85490197,  0.83137256,  0.84313726, ...,  0.60392159,\n",
       "           0.60000002,  0.61176473]],\n",
       "\n",
       "        [[ 0.28235295,  0.10980392,  0.76862746, ...,  0.03529412,\n",
       "           0.08235294,  0.0627451 ],\n",
       "         [ 0.7019608 ,  0.97647059,  0.98431373, ...,  0.38039216,\n",
       "           0.47843137,  0.10980392],\n",
       "         [ 0.96470588,  0.98039216,  0.98431373, ...,  0.30980393,\n",
       "           0.53725493,  0.60392159],\n",
       "         ..., \n",
       "         [ 0.80392158,  0.80000001,  0.81568629, ...,  0.59607846,\n",
       "           0.60392159,  0.60784316],\n",
       "         [ 0.79215688,  0.84313726,  0.80392158, ...,  0.63137257,\n",
       "           0.60392159,  0.61960787],\n",
       "         [ 0.82745099,  0.80784315,  0.81960785, ...,  0.6156863 ,\n",
       "           0.61176473,  0.62352943]]],\n",
       "\n",
       "\n",
       "       [[[ 0.03529412,  0.03921569,  0.02745098, ...,  0.03921569,\n",
       "           0.03137255,  0.05098039],\n",
       "         [ 0.02745098,  0.03137255,  0.03137255, ...,  0.03137255,\n",
       "           0.03529412,  0.04705882],\n",
       "         [ 0.02352941,  0.02745098,  0.02745098, ...,  0.04313726,\n",
       "           0.04313726,  0.04313726],\n",
       "         ..., \n",
       "         [ 0.28627452,  0.21960784,  0.09803922, ...,  0.02745098,\n",
       "           0.03921569,  0.05882353],\n",
       "         [ 0.26274511,  0.11372549,  0.07058824, ...,  0.07058824,\n",
       "           0.03921569,  0.04313726],\n",
       "         [ 0.23529412,  0.19607843,  0.16078432, ...,  0.02745098,\n",
       "           0.04705882,  0.04705882]],\n",
       "\n",
       "        [[ 0.02745098,  0.03137255,  0.01960784, ...,  0.04313726,\n",
       "           0.03137255,  0.03137255],\n",
       "         [ 0.01960784,  0.02352941,  0.02352941, ...,  0.03529412,\n",
       "           0.03529412,  0.03137255],\n",
       "         [ 0.01568628,  0.01960784,  0.01960784, ...,  0.03921569,\n",
       "           0.04313726,  0.03137255],\n",
       "         ..., \n",
       "         [ 0.49803922,  0.38039216,  0.2       , ...,  0.02352941,\n",
       "           0.03529412,  0.04705882],\n",
       "         [ 0.49411765,  0.30980393,  0.1882353 , ...,  0.05882353,\n",
       "           0.03921569,  0.04313726],\n",
       "         [ 0.48235294,  0.41176471,  0.30980393, ...,  0.03137255,\n",
       "           0.04705882,  0.04705882]],\n",
       "\n",
       "        [[ 0.02745098,  0.03137255,  0.01960784, ...,  0.48235294,\n",
       "           0.49019608,  0.48235294],\n",
       "         [ 0.01960784,  0.02352941,  0.02352941, ...,  0.47450981,\n",
       "           0.48235294,  0.47450981],\n",
       "         [ 0.01568628,  0.01960784,  0.01960784, ...,  0.47058824,\n",
       "           0.46666667,  0.4627451 ],\n",
       "         ..., \n",
       "         [ 0.73333335,  0.60000002,  0.3882353 , ...,  0.25098041,\n",
       "           0.13725491,  0.10980392],\n",
       "         [ 0.73333335,  0.54509807,  0.37254903, ...,  0.18431373,\n",
       "           0.08627451,  0.11372549],\n",
       "         [ 0.71764708,  0.64313728,  0.50196081, ...,  0.11764706,\n",
       "           0.09411765,  0.11764706]]],\n",
       "\n",
       "\n",
       "       ..., \n",
       "       [[[ 0.91764706,  0.97254902,  0.74901962, ...,  0.32549021,\n",
       "           0.24705882,  0.29019609],\n",
       "         [ 0.79607844,  0.97254902,  0.7019608 , ...,  0.25490198,\n",
       "           0.27450982,  0.25490198],\n",
       "         [ 0.73333335,  0.93333334,  0.72549021, ...,  0.23137255,\n",
       "           0.28235295,  0.21568628],\n",
       "         ..., \n",
       "         [ 0.24313726,  0.24313726,  0.24313726, ...,  0.2       ,\n",
       "           0.09411765,  0.12156863],\n",
       "         [ 0.23137255,  0.23529412,  0.25098041, ...,  0.18039216,\n",
       "           0.23137255,  0.12941177],\n",
       "         [ 0.24313726,  0.25098041,  0.28627452, ...,  0.18039216,\n",
       "           0.22352941,  0.1254902 ]],\n",
       "\n",
       "        [[ 0.89411765,  0.87058824,  0.59607846, ...,  0.43137255,\n",
       "           0.35294119,  0.40000001],\n",
       "         [ 0.77254903,  0.87058824,  0.54509807, ...,  0.35686275,\n",
       "           0.38039216,  0.36470589],\n",
       "         [ 0.67450982,  0.83137256,  0.57647061, ...,  0.33333334,\n",
       "           0.3882353 ,  0.32941177],\n",
       "         ..., \n",
       "         [ 0.17647059,  0.17647059,  0.17647059, ...,  0.32941177,\n",
       "           0.18039216,  0.17254902],\n",
       "         [ 0.2       ,  0.18431373,  0.18431373, ...,  0.3137255 ,\n",
       "           0.32549021,  0.17647059],\n",
       "         [ 0.18431373,  0.17254902,  0.19215687, ...,  0.3137255 ,\n",
       "           0.31764707,  0.16470589]],\n",
       "\n",
       "        [[ 0.87058824,  0.80784315,  0.50588238, ...,  0.57647061,\n",
       "           0.49411765,  0.52156866],\n",
       "         [ 0.75686276,  0.81568629,  0.43137255, ...,  0.49803922,\n",
       "           0.50196081,  0.48627451],\n",
       "         [ 0.63921571,  0.78431374,  0.4509804 , ...,  0.4509804 ,\n",
       "           0.49411765,  0.43529412],\n",
       "         ..., \n",
       "         [ 0.23137255,  0.23137255,  0.23137255, ...,  0.43137255,\n",
       "           0.23921569,  0.20392157],\n",
       "         [ 0.23921569,  0.24705882,  0.22745098, ...,  0.42745098,\n",
       "           0.41176471,  0.2       ],\n",
       "         [ 0.23137255,  0.23529412,  0.22745098, ...,  0.42745098,\n",
       "           0.40392157,  0.19607843]]],\n",
       "\n",
       "\n",
       "       [[[ 0.75686276,  0.74117649,  0.80000001, ...,  0.77254903,\n",
       "           0.75686276,  0.7764706 ],\n",
       "         [ 0.7647059 ,  0.74901962,  0.78039217, ...,  0.7647059 ,\n",
       "           0.75294119,  0.69803923],\n",
       "         [ 0.72156864,  0.74509805,  0.7764706 , ...,  0.60000002,\n",
       "           0.56078434,  0.56862748],\n",
       "         ..., \n",
       "         [ 0.0627451 ,  0.06666667,  0.05882353, ...,  0.34509805,\n",
       "           0.34509805,  0.34509805],\n",
       "         [ 0.09411765,  0.07450981,  0.05882353, ...,  0.33725491,\n",
       "           0.34509805,  0.33725491],\n",
       "         [ 0.08235294,  0.07058824,  0.07450981, ...,  0.34509805,\n",
       "           0.31764707,  0.32941177]],\n",
       "\n",
       "        [[ 0.73725492,  0.73333335,  0.79607844, ...,  0.81960785,\n",
       "           0.80392158,  0.82352942],\n",
       "         [ 0.74509805,  0.74117649,  0.7764706 , ...,  0.80392158,\n",
       "           0.79215688,  0.72941178],\n",
       "         [ 0.7019608 ,  0.73725492,  0.77254903, ...,  0.61960787,\n",
       "           0.58039218,  0.58039218],\n",
       "         ..., \n",
       "         [ 0.09803922,  0.10196079,  0.09411765, ...,  0.56078434,\n",
       "           0.54901963,  0.54901963],\n",
       "         [ 0.12941177,  0.10980392,  0.09411765, ...,  0.5529412 ,\n",
       "           0.54901963,  0.5411765 ],\n",
       "         [ 0.09803922,  0.08627451,  0.09019608, ...,  0.56078434,\n",
       "           0.53333336,  0.5411765 ]],\n",
       "\n",
       "        [[ 0.7019608 ,  0.7019608 ,  0.78039217, ...,  0.84313726,\n",
       "           0.82745099,  0.82352942],\n",
       "         [ 0.70980394,  0.70980394,  0.76078433, ...,  0.80392158,\n",
       "           0.79215688,  0.72549021],\n",
       "         [ 0.66666669,  0.70588237,  0.75686276, ...,  0.61176473,\n",
       "           0.57254905,  0.57254905],\n",
       "         ..., \n",
       "         [ 0.13725491,  0.14117648,  0.13333334, ...,  0.73725492,\n",
       "           0.72941178,  0.72941178],\n",
       "         [ 0.16862746,  0.14901961,  0.13333334, ...,  0.72941178,\n",
       "           0.72941178,  0.72156864],\n",
       "         [ 0.14117648,  0.12941177,  0.13333334, ...,  0.73725492,\n",
       "           0.70980394,  0.72549021]]],\n",
       "\n",
       "\n",
       "       [[[ 0.03137255,  0.03137255,  0.03137255, ...,  0.0627451 ,\n",
       "           0.03529412,  0.04313726],\n",
       "         [ 0.03137255,  0.03137255,  0.03137255, ...,  0.0627451 ,\n",
       "           0.09803922,  0.05490196],\n",
       "         [ 0.03137255,  0.03921569,  0.03529412, ...,  0.08235294,\n",
       "           0.07450981,  0.0627451 ],\n",
       "         ..., \n",
       "         [ 0.06666667,  0.07058824,  0.1254902 , ...,  0.03137255,\n",
       "           0.04313726,  0.03529412],\n",
       "         [ 0.0627451 ,  0.07843138,  0.10980392, ...,  0.03921569,\n",
       "           0.03137255,  0.03137255],\n",
       "         [ 0.06666667,  0.07058824,  0.10196079, ...,  0.05490196,\n",
       "           0.02745098,  0.05490196]],\n",
       "\n",
       "        [[ 0.03529412,  0.03529412,  0.03529412, ...,  0.08627451,\n",
       "           0.05882353,  0.06666667],\n",
       "         [ 0.03529412,  0.03529412,  0.03529412, ...,  0.05490196,\n",
       "           0.09019608,  0.07843138],\n",
       "         [ 0.03529412,  0.04313726,  0.03921569, ...,  0.07843138,\n",
       "           0.10980392,  0.07058824],\n",
       "         ..., \n",
       "         [ 0.07843138,  0.08235294,  0.09803922, ...,  0.04313726,\n",
       "           0.05490196,  0.04705882],\n",
       "         [ 0.07450981,  0.09019608,  0.10980392, ...,  0.05098039,\n",
       "           0.04313726,  0.04313726],\n",
       "         [ 0.07843138,  0.08235294,  0.10196079, ...,  0.06666667,\n",
       "           0.03921569,  0.06666667]],\n",
       "\n",
       "        [[ 0.05098039,  0.05098039,  0.05098039, ...,  0.07843138,\n",
       "           0.04313726,  0.05490196],\n",
       "         [ 0.05098039,  0.05098039,  0.05098039, ...,  0.08627451,\n",
       "           0.11372549,  0.10196079],\n",
       "         [ 0.05098039,  0.05882353,  0.05490196, ...,  0.05882353,\n",
       "           0.11764706,  0.04705882],\n",
       "         ..., \n",
       "         [ 0.09411765,  0.09803922,  0.11764706, ...,  0.05882353,\n",
       "           0.07058824,  0.0627451 ],\n",
       "         [ 0.09411765,  0.10980392,  0.13333334, ...,  0.06666667,\n",
       "           0.05882353,  0.05882353],\n",
       "         [ 0.09803922,  0.10196079,  0.1254902 , ...,  0.08235294,\n",
       "           0.05490196,  0.08235294]]]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Keras sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajinkya.parkar@ibm.com/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(32, kernel_size=(3, 3),padding='same',input_shape=(3 , 100, 100)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution2D(64,(3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(9))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model.load_weights(\"weights.16-0.86800.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's train the model using SGD + momentum (how original).\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, ..., 1, 0, 0],\n",
       "       [0, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 1, 0, ..., 1, 0, 1],\n",
       "       ..., \n",
       "       [0, 1, 1, ..., 1, 1, 0],\n",
       "       [0, 1, 0, ..., 1, 0, 0],\n",
       "       [0, 1, 1, ..., 1, 1, 1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "check = ModelCheckpoint(\"weights.{epoch:02d}-{val_acc:.5f}.hdf5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, mode='auto')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajinkya.parkar@ibm.com/anaconda/lib/python3.6/site-packages/keras/models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 164389 samples, validate on 35226 samples\n",
      "Epoch 1/20\n",
      "164300/164389 [============================>.] - ETA: 8s - loss: 0.5973 - acc: 0.6725 Epoch 00000: val_acc improved from -inf to 0.69199, saving model to weights.00-0.69199.hdf5\n",
      "164389/164389 [==============================] - 18209s - loss: 0.5973 - acc: 0.6725 - val_loss: 0.5722 - val_acc: 0.6920\n",
      "Epoch 2/20\n",
      "164300/164389 [============================>.] - ETA: 8s - loss: 0.5713 - acc: 0.6929 Epoch 00001: val_acc improved from 0.69199 to 0.70082, saving model to weights.01-0.70082.hdf5\n",
      "164389/164389 [==============================] - 17867s - loss: 0.5713 - acc: 0.6929 - val_loss: 0.5606 - val_acc: 0.7008\n",
      "Epoch 3/20\n",
      "164300/164389 [============================>.] - ETA: 9s - loss: 0.5611 - acc: 0.7005 Epoch 00002: val_acc improved from 0.70082 to 0.70622, saving model to weights.02-0.70622.hdf5\n",
      "164389/164389 [==============================] - 18527s - loss: 0.5611 - acc: 0.7005 - val_loss: 0.5522 - val_acc: 0.7062\n",
      "Epoch 4/20\n",
      "164300/164389 [============================>.] - ETA: 9s - loss: 0.5543 - acc: 0.7054 Epoch 00003: val_acc did not improve\n",
      "164389/164389 [==============================] - 18658s - loss: 0.5543 - acc: 0.7054 - val_loss: 0.5551 - val_acc: 0.7041\n",
      "Epoch 5/20\n",
      "164300/164389 [============================>.] - ETA: 8s - loss: 0.5487 - acc: 0.7093 Epoch 00004: val_acc improved from 0.70622 to 0.71288, saving model to weights.04-0.71288.hdf5\n",
      "164389/164389 [==============================] - 18363s - loss: 0.5487 - acc: 0.7093 - val_loss: 0.5437 - val_acc: 0.7129\n",
      "Epoch 6/20\n",
      "164300/164389 [============================>.] - ETA: 8s - loss: 0.5438 - acc: 0.7129 Epoch 00005: val_acc did not improve\n",
      "164389/164389 [==============================] - 18200s - loss: 0.5438 - acc: 0.7129 - val_loss: 0.5468 - val_acc: 0.7111\n",
      "Epoch 7/20\n",
      "164300/164389 [============================>.] - ETA: 8s - loss: 0.5391 - acc: 0.7167 Epoch 00006: val_acc improved from 0.71288 to 0.71710, saving model to weights.06-0.71710.hdf5\n",
      "164389/164389 [==============================] - 18273s - loss: 0.5391 - acc: 0.7167 - val_loss: 0.5379 - val_acc: 0.7171\n",
      "Epoch 8/20\n",
      "164300/164389 [============================>.] - ETA: 8s - loss: 0.5338 - acc: 0.7205 Epoch 00007: val_acc improved from 0.71710 to 0.71883, saving model to weights.07-0.71883.hdf5\n",
      "164389/164389 [==============================] - 18120s - loss: 0.5338 - acc: 0.7205 - val_loss: 0.5352 - val_acc: 0.7188\n",
      "Epoch 9/20\n",
      "164300/164389 [============================>.] - ETA: 8s - loss: 0.5292 - acc: 0.7243 Epoch 00008: val_acc improved from 0.71883 to 0.71999, saving model to weights.08-0.71999.hdf5\n",
      "164389/164389 [==============================] - 18187s - loss: 0.5291 - acc: 0.7244 - val_loss: 0.5331 - val_acc: 0.7200\n",
      "Epoch 10/20\n",
      "164300/164389 [============================>.] - ETA: 8s - loss: 0.5234 - acc: 0.7282 Epoch 00009: val_acc improved from 0.71999 to 0.72228, saving model to weights.09-0.72228.hdf5\n",
      "164389/164389 [==============================] - 18247s - loss: 0.5235 - acc: 0.7281 - val_loss: 0.5308 - val_acc: 0.7223\n",
      "Epoch 11/20\n",
      "164300/164389 [============================>.] - ETA: 8s - loss: 0.5169 - acc: 0.7328 Epoch 00010: val_acc did not improve\n",
      "164389/164389 [==============================] - 18384s - loss: 0.5169 - acc: 0.7328 - val_loss: 0.5309 - val_acc: 0.7216\n",
      "Epoch 12/20\n",
      "164300/164389 [============================>.] - ETA: 9s - loss: 0.5098 - acc: 0.7382 Epoch 00011: val_acc improved from 0.72228 to 0.72365, saving model to weights.11-0.72365.hdf5\n",
      "164389/164389 [==============================] - 18427s - loss: 0.5099 - acc: 0.7382 - val_loss: 0.5283 - val_acc: 0.7236\n",
      "Epoch 13/20\n",
      "164300/164389 [============================>.] - ETA: 8s - loss: 0.5010 - acc: 0.7445 Epoch 00012: val_acc did not improve\n",
      "164389/164389 [==============================] - 18401s - loss: 0.5010 - acc: 0.7445 - val_loss: 0.5304 - val_acc: 0.7224\n",
      "Epoch 14/20\n",
      "164300/164389 [============================>.] - ETA: 9s - loss: 0.4912 - acc: 0.7516 Epoch 00013: val_acc did not improve\n",
      "164389/164389 [==============================] - 18522s - loss: 0.4912 - acc: 0.7516 - val_loss: 0.5303 - val_acc: 0.7221\n",
      "Epoch 15/20\n",
      "164300/164389 [============================>.] - ETA: 8s - loss: 0.4798 - acc: 0.7595 Epoch 00014: val_acc did not improve\n",
      "164389/164389 [==============================] - 18158s - loss: 0.4798 - acc: 0.7595 - val_loss: 0.5315 - val_acc: 0.7210\n",
      "Epoch 16/20\n",
      "164300/164389 [============================>.] - ETA: 8s - loss: 0.4666 - acc: 0.7687 Epoch 00015: val_acc did not improve\n",
      "164389/164389 [==============================] - 21974s - loss: 0.4666 - acc: 0.7687 - val_loss: 0.5321 - val_acc: 0.7225\n",
      "Epoch 17/20\n",
      "164300/164389 [============================>.] - ETA: 9s - loss: 0.4524 - acc: 0.7776 Epoch 00016: val_acc did not improve\n",
      "164389/164389 [==============================] - 19021s - loss: 0.4524 - acc: 0.7776 - val_loss: 0.5375 - val_acc: 0.7224\n",
      "Epoch 18/20\n",
      "164300/164389 [============================>.] - ETA: 14s - loss: 0.4367 - acc: 0.7877Epoch 00017: val_acc did not improve\n",
      "164389/164389 [==============================] - 27914s - loss: 0.4367 - acc: 0.7877 - val_loss: 0.5447 - val_acc: 0.7172\n",
      "Epoch 19/20\n",
      "164300/164389 [============================>.] - ETA: 9s - loss: 0.4209 - acc: 0.7971 Epoch 00018: val_acc did not improve\n",
      "164389/164389 [==============================] - 18638s - loss: 0.4209 - acc: 0.7971 - val_loss: 0.5474 - val_acc: 0.7185\n",
      "Epoch 20/20\n",
      "164300/164389 [============================>.] - ETA: 9s - loss: 0.4060 - acc: 0.8062 Epoch 00019: val_acc did not improve\n",
      "164389/164389 [==============================] - 18695s - loss: 0.4061 - acc: 0.8062 - val_loss: 0.5521 - val_acc: 0.7195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2145c3da0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=100, nb_epoch=20,callbacks=[check],validation_data=(x_val,y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05882353,  0.07450981,  0.05882353,  0.05490196,  0.06666667,\n",
       "        0.08627451,  0.07843138,  0.07843138,  0.08235294,  0.09411765,\n",
       "        0.09803922,  0.10588235,  0.11372549,  0.12156863,  0.1254902 ,\n",
       "        0.12156863,  0.12941177,  0.14117648,  0.14117648,  0.13725491,\n",
       "        0.14117648,  0.15686275,  0.16078432,  0.15294118,  0.15686275,\n",
       "        0.14509805,  0.14509805,  0.14117648,  0.14117648,  0.14901961,\n",
       "        0.07450981,  0.14509805,  0.14117648,  0.15686275,  0.14509805,\n",
       "        0.15294118,  0.15686275,  0.16078432,  0.16862746,  0.16862746,\n",
       "        0.17254902,  0.17647059,  0.17254902,  0.16470589,  0.15686275,\n",
       "        0.16470589,  0.17647059,  0.17647059,  0.17647059,  0.17647059,\n",
       "        0.16470589,  0.16470589,  0.16862746,  0.16862746,  0.18039216,\n",
       "        0.18039216,  0.18039216,  0.18039216,  0.18039216,  0.18039216,\n",
       "        0.18039216,  0.18039216,  0.17647059,  0.17254902,  0.17254902,\n",
       "        0.17254902,  0.17254902,  0.16862746,  0.16862746,  0.17254902,\n",
       "        0.17254902,  0.16862746,  0.16078432,  0.16078432,  0.16078432,\n",
       "        0.16862746,  0.15686275,  0.14901961,  0.15686275,  0.16078432,\n",
       "        0.16078432,  0.16470589,  0.15686275,  0.16078432,  0.15686275,\n",
       "        0.14509805,  0.14509805,  0.13725491,  0.13333334,  0.13333334,\n",
       "        0.14509805,  0.14117648,  0.14117648,  0.14117648,  0.14901961,\n",
       "        0.13725491,  0.13725491,  0.1254902 ,  0.12156863,  0.10980392], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 1, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.52941179,  0.5411765 ,  0.50196081,  0.49411765,  0.40392157,\n",
       "        0.43529412,  0.42745098,  0.41176471,  0.42745098,  0.43921569,\n",
       "        0.49411765,  0.44705883,  0.41960785,  0.41960785,  0.41960785,\n",
       "        0.44705883,  0.4509804 ,  0.41568628,  0.42745098,  0.44705883,\n",
       "        0.45490196,  0.40000001,  0.32549021,  0.23921569,  0.4509804 ,\n",
       "        0.43137255,  0.45490196,  0.45490196,  0.4627451 ,  0.4627451 ,\n",
       "        0.47058824,  0.45490196,  0.47450981,  0.4627451 ,  0.4627451 ,\n",
       "        0.4509804 ,  0.44313726,  0.4509804 ,  0.45882353,  0.45882353,\n",
       "        0.44705883,  0.47058824,  0.47843137,  0.4509804 ,  0.47058824,\n",
       "        0.49803922,  0.51372552,  0.4627451 ,  0.53725493,  0.54509807,\n",
       "        0.45490196,  0.47058824,  0.4509804 ,  0.46666667,  0.49803922,\n",
       "        0.49019608,  0.51372552,  0.51372552,  0.48627451,  0.49803922,\n",
       "        0.50196081,  0.51764709,  0.54901963,  0.5529412 ,  0.50980395,\n",
       "        0.50980395,  0.49019608,  0.52941179,  0.49019608,  0.52156866,\n",
       "        0.35686275,  0.40392157,  0.40392157,  0.40392157,  0.40000001,\n",
       "        0.40000001,  0.41568628,  0.3764706 ,  0.38039216,  0.36470589,\n",
       "        0.38431373,  0.38431373,  0.3882353 ,  0.40784314,  0.40784314,\n",
       "        0.3764706 ,  0.36862746,  0.3882353 ,  0.3764706 ,  0.39607844,\n",
       "        0.35686275,  0.33725491,  0.33725491,  0.36078432,  0.35686275,\n",
       "        0.36470589,  0.36470589,  0.34901962,  0.34901962,  0.32941177], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_24 (Conv2D)           (None, 32, 100, 100)      896       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 32, 100, 100)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 64, 98, 98)        18496     \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 64, 98, 98)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 64, 49, 49)        0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 64, 49, 49)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 64, 49, 49)        36928     \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 64, 49, 49)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 64, 47, 47)        36928     \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 64, 47, 47)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 64, 23, 23)        0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 64, 23, 23)        0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 33856)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 512)               17334784  \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 5)                 2565      \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 17,430,597\n",
      "Trainable params: 17,430,597\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
